首先我们读入数据：
```{r}
data <- read.csv(file = "/Users/andreolli/Desktop/BodyFat.csv")
data <- data[, -c(1,3)]
```

第182行的BODYFAT为0显然是错误的，我们在之后的分析中去掉第182行数据。
*********
用str函数可以查看data各列的数据类型
```{r}
str(data)
```
我们发现除了变量AGE为int型以外，其他变量都为浮点型数据，下面我们首先要考虑的是AGE是否需要作为因子型变量加入到模型中。我们首先画出AGE对BODYFAT的图：
```{r}
plot(y= data$BODYFAT, x = data$AGE)
```
从图中我们可以近似看到AGE与BODYFAT近似有线性关系，这提示我们AGE不需要作为因子型变量出现，下面我们更加精确的验证这一个论断：
```{r}
model.age <- lm(BODYFAT ~ AGE, data = data[-182,])
summary(model.age)
plot(model.age)
```
异常点为216， 192， 36

```{r}
hat.plot <- function(fit){
  p <- length(coefficients(fit))
  n <- length(fitted(fit))
  plot(hatvalues(fit),main = "Index Plot of Hat Values")
  abline(h=c(2,3)*p/n,col="red",lty=2)
  identify(1:n, hatvalues(fit), names(hatvalues(fit)))  
  #这句产生交互效果，选中某个点后，关闭后返回点的名称
}
hat.plot(model.age)
```
强影响力点为79， 252. 我们去掉异常点再建立一次模型

```{r}
data.age <- data[-c(79, 252, 36, 216, 182, 192),]
model.age.2 <- lm(BODYFAT ~ AGE, data = data.age)
summary(model.age.2)
plot(model.age.2)
```
现在我们可以看到AGE依然显著，同时残差图没有模式，qq图显示正态性假设近似成立，同时没有明显异常点，这说明我们确实可以将AGE直接加入模型中。


下面， 我们尝试建立BODYFAT对于其他所有自变量的线性回归模型，查看其效果：

```{r}
model <- lm(BODYFAT ~ ., data = data[-182, ])
summary(model)
plot(model)
```
我们首先检验模型中是否存在多重共线性：
```{r}
require(car)
vif(model)
```

我们发现模型中存在着严重的多重共线性问题，同时图形显示模型中有明显的异常值点，需要去掉。由于多重共线性不会影响异常值点的诊断，我们首先去掉异常点。明显的，我们首先去掉点39， 207， 224, 再检验高杠杆值点：

```{r}
hat.plot(model)
```
为 31 36 39  41  42 86 159 175 206，去掉现有异常点在此建立模型：


```{r}
data2 <- data[-c(182, 31, 36, 39, 41, 42, 86, 159, 175, 206, 207, 297, 224),]
model2 <- lm(BODYFAT ~ ., data = data2)
plot(model2)
```


检验高杠杆值点：
```{r}
hat.plot(model2)
```
为49，155，205，210 ， 216, 221, 54, 163(54, 163为明显的在residual vs leverage图中未标出来的靠右的点)



去掉现有异常点并在此建立模型：
```{r}
data3 <- data[-c(182, 31, 36, 39, 41, 42, 86, 106, 159, 175, 206, 207, 297, 224, 49, 155, 205, 210, 216, 230, 216, 221, 54, 163),]
model3 <- lm(BODYFAT ~ ., data = data3)
plot(model3)
```

我们可以看出在模型中没有明显的强杠杆值点和异常点了，但是正态qq图显示正态性假定可能不满足，我们检验这一点：

```{r}
shapiro.test(model3$residuals) 
```

检验显示正态性假定未满足，但是偏离程度比较低。我们先尝试进行box-cox变换：

```{r}
data.trans <- data[-c(182, 31, 36, 39, 41, 42, 86, 106, 159, 175, 206, 207, 297, 224, 49, 155, 205, 210, 216, 230, 216, 221, 54, 163),]
summary(p1 <- powerTransform(data.trans$BODYFAT))
```

选取power为0.9
```{r}
model.trans <- lm(BODYFAT^0.9 ~ ., data = data3)
plot(model.trans)
```
发现经过box-cox变换后残差依然不满足正态性假定。但是，在线性回归分析中我们学过，当独立性假定，线性性假定， 同方差性假定满足，并且误差项分布与正态分布比较相近时，线性回归模型依然是稳健的。我们下面首先验证前面的各个假定，其中所用的方法在R语言实战书中有：

独立性假定
```{r}
durbinWatsonTest(model3)
```
Durbin-Watson检验的p值非常高，可以看到独立性假定很好的满足了。

同方差性假定
```{r}
ncvTest(model3)
```
计分检验p值非常高，同方差性满足

线性性假定和误差项的偏度，峰度与正态偏度峰度相近，所用函数在中文版R语言实战第181页
```{r}
require(gvlma)
gvmodel <- gvlma(model3)
summary(gvmodel)
```
可以看到线性性假定满足，同时残差偏度峰度与正态分布偏度峰度类似，结合之前正态性检验的p值为0.11，p值不大，说明线性模型比较稳健，我们可以使用线性模型。





下面需要处理多重共线性问题。我们考虑四种不同的方法：1.lasso regression， 2.主成分分析 3.偏最小二乘。之后我们用k-fold交叉检验方法从中选出最佳的模型。论文显示k可以取5.

首先我们考虑lasso regression模型，用glmnet包（lasso法发明人写的包），介绍在https://cosx.org/2016/10/data-mining-1-lasso/

```{r}
require(glmnet)
y = as.matrix(data3[,1])
x = as.matrix(data3[,-1])
fit = cv.glmnet(x = x, y = y, family = "gaussian", nlambda = 100, alpha = 1, standardize=TRUE, nfolds = 10)
print(fit$lambda.1se)
plot(fit)
```
cv.glmnet函数自带了交叉检验选择最佳的lambda，这里我们可以看到使用lasso方法时我们保留四个变量，同时可以设置lambda值为0.4。下面我们确定保留哪些变量：
```{r}
fit2 = glmnet(x = x, y = y, family = "gaussian", nlambda = 100, alpha = 1, standardize=TRUE)
plot(fit2, xvar = "lambda", label = TRUE)

```
可以看到第1， 3， 7， 14个自变量被保留，我们把对应的数据存入data.lasso中并拟合模型：

```{r}
data.lasso <- data3[,c(1,2,4,8,15)]
model.lasso <- lm(BODYFAT ~., data = data.lasso)
summary(model.lasso)
plot(model.lasso)
vif(model.lasso)
```
可以看到模型的各项假定除了正态性假定仍有轻微偏离以外其他假定都很好的满足了，同时各个变量都显著，且方差膨胀因子足够小，多重共线性问题解决了。

对数据标准化以便建立主成分分析模型和偏最小二乘模型：
```{r}
data.scaled <- as.data.frame(scale(data3, center = TRUE, scale = TRUE))
```

下面考虑主成分分析模型，所用函数介绍在R语言实战第十四章，要安装psych包：
```{r}
require(psych)
```

fa.parallel函数可以确定主成分的个数：
```{r}
fa.parallel(data.scaled[,-1], fa = "pc", n.iter = 1000, show.legend = FALSE, main = "Scree plot with parallel analysis")
```
碎石图展示了R语言实战第三版中的三种不同判别主成分个数的法则所推荐的主成分个数，这里第二， 第三种准则都推荐使用2个主成分。第一种准则可推荐2， 3，4， 5个主成分，下面分别对不同主成分数量提取主成分并进行测试：
```{r}
principal(data.scaled[,-1], nfactors = 2)
principal(data.scaled[,-1], nfactors = 3)
principal(data.scaled[,-1], nfactors = 4)
principal(data.scaled[,-1], nfactors = 5)
```
u2行表示了方差无法被主成分表示的比例， 使用三个主成分而不是两个时height变量无法被解释的比例大幅降低，使用五个而不是四个时ankle变量无法被解释的比例大幅降低。为了判断最佳的主成分个数，稍后我们会用k-fold交叉验证方法选出最适合预测问题的主成分个数。

下面用偏最小二乘法建立模型， 要安装pls包：
```{r}
require("pls")
```


```{r}
fit.pls <- plsr(BODYFAT~.,data=data.scaled,scale=T,validation="CV")
summary(fit.pls)
```

由于每次运行时所得的adjCV的值不一样，经过多次重复后发现我们可以使用2到5个成分，其中使用2到3个成分即可很好的解释因变量方差。下面我们分别对lasso回归得到的模型，2到5个主成分的主成分分析模型，2到5个主成分的偏最小二乘模型用k-fold交叉验证方法选出最好的模型。为了保证cv得出的结果的数量级一致，我们都用正规化后的数据进行分析：

安装boot包
```{r}
require(boot)

```
对lasso模型用10-fold 交叉验证 100次并计算adjusted cv的平均值：
```{r}
  data.lasso.scaled <- as.data.frame(scale(data.lasso, center = T, scale = T))
cv.err <- rep(0,30)
for(i in 1:30){

  glm.fit <- glm(BODYFAT ~ .,data = data.lasso.scaled)
  cv.err[i] = cv.glm(data.lasso.scaled,glm.fit,K=10)$delta[2]
}
plot(cv.err)
print(mean(cv.err))
```

对主成分分析模型进行交叉检验的函数，n为主成分数
```{r}
get.cv.prin <- function(n)
{
covmatrix <- cov(data.scaled[,-1])
eigen.vector <- eigen(covmatrix)$vectors
eigen.vector.n <- eigen.vector[,1:n]
prin.data.n <- as.matrix(data.scaled[,-1]) %*% eigen.vector.n
data.frame.n <- as.data.frame(cbind(data.scaled[,1], prin.data.n))
cv.err <- rep(0,30)
for(i in 1:30) {
  glm.fit <- glm(V1 ~ V2 + V3, data = data.frame.n)
  cv.err[i] = cv.glm(data.frame.n,glm.fit,K=10)$delta[2]
}
plot(cv.err)
print(mean(cv.err))
}
```


```{r}
get.cv.prin(2)
get.cv.prin(3)
get.cv.prin(4)
get.cv.prin(5)

```
pls的cv在之前已经给出来了，最后发现竟然是lasso优于主成分优于偏最小二乘？我觉得交叉检验这一块有问题做的很不好，先来个初稿把
```{r}


```

```{r}


```

```{r}


```

```{r}


```


```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```


```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```


```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```